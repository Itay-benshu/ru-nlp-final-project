{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd9e6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\mambaforge\\envs\\torch\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\ProgramData\\mambaforge\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer, ViTImageProcessor, VisionEncoderDecoderModel, VisionEncoderDecoderConfig, AutoConfig\n",
    "from split import split_dataset\n",
    "from dataset import SongsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from training import train_model\n",
    "from evaluation import evaluate_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f233921",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83ccf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "DECODER_NAME = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962be834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "all_songs_path = 'data/songs/all'\n",
    "# split_dataset(all_songs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807a9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image_processor and tokenizer to be used in datasets\n",
    "# image_processor = ViTImageProcessor.from_pretrained(ENCODER_NAME)\n",
    "image_processor = ViTImageProcessor.from_pretrained(ENCODER_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(DECODER_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0b4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a264923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 1990\n",
      "val size 22\n",
      "test size 25\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "train_dataset = SongsDataset('data/songs/train', image_processor, tokenizer, by_line=False, n_variations=10)\n",
    "print('train size', len(train_dataset))\n",
    "validation_dataset = SongsDataset('data/songs/validation', image_processor, tokenizer, by_line=False, n_variations=1)\n",
    "print('val size', len(validation_dataset))\n",
    "test_dataset = SongsDataset('data/songs/test', image_processor, tokenizer, by_line=False, n_variations=1)\n",
    "print('test size', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488e2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e3921eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from 'C:\\\\Users\\\\Itay\\\\Documents\\\\GitHub\\\\nlp-final-project\\\\evaluation.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import training\n",
    "import evaluation\n",
    "from training import train_model\n",
    "from evaluation import evaluate_model\n",
    "importlib.reload(training)\n",
    "importlib.reload(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.5.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.6.ln_cross_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.4.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.7.ln_cross_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.3.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.11.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.9.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.7.ln_cross_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.11.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.8.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5 - Loss: 3.1683:  27%|███████████████                                         | 17/63 [00:16<00:39,  1.17it/s]"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER_NAME, DECODER_NAME)\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "\n",
    "train_history, val_history = train_model(model, train_dataloader, validation_dataloader, \n",
    "                                         num_epochs=5, learning_rate=1e-4, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7d523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"best_so_far.chk\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd17bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search parameters\n",
    "model.config.max_length = 32\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "true_lyrics, predicted_lyrics = evaluate_model(model, test_dataloader, device=DEVICE, \n",
    "                                               max_new_tokens=32, \n",
    "                                               num_beams=4, \n",
    "                                               no_repeat_ngram_size=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c5119",
   "metadata": {},
   "source": [
    "### Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de05594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "484cf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = [k['label'] for k in sentiment_pipeline(predicted_lyrics)]\n",
    "true_sentiments = [k['label'] for k in sentiment_pipeline(true_lyrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0f6edb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_sentiments, predicted_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60b700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
